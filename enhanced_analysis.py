"""
Enhanced Company Intelligence Analysis
SDS Datathon 2026 - Competitive Version

This script performs:
1. Multi-dimensional clustering (Revenue, Employees, Entity Type, Industry)
2. B2B Lead Score calculation (0-100)
3. Industry benchmarking
4. Risk signal detection
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.cluster import KMeans
from sklearn.ensemble import IsolationForest
from sklearn.metrics import silhouette_score
import warnings
warnings.filterwarnings('ignore')

print("=" * 60)
print("ðŸš€ ENHANCED COMPANY INTELLIGENCE ANALYSIS")
print("   SDS Datathon 2026 - Competitive Version")
print("=" * 60)

# ============================================================
# 1. DATA LOADING & CLEANING
# ============================================================
print("\nðŸ“‚ Loading data...")
df = pd.read_csv('champions_group_data.csv')
print(f"   Loaded {len(df):,} companies")

# Clean numeric columns
def clean_numeric(col):
    return pd.to_numeric(df[col].astype(str).str.replace(',', '').str.strip(), errors='coerce').fillna(0)

df['Revenue_USD_Clean'] = clean_numeric('Revenue (USD)')
df['Employees_Total_Clean'] = clean_numeric('Employees Total')
df['Employees_Site_Clean'] = clean_numeric('Employees Single Site')
df['Corporate_Family_Size'] = clean_numeric('Corporate Family Members')

# ============================================================
# 2. FEATURE ENGINEERING
# ============================================================
print("\nðŸ”§ Engineering features...")

# Entity Type Score (decision-making power)
entity_map = {
    'Headquarters': 4,
    'Single Location': 3,
    'Subsidiary': 2,
    'Branch': 1
}
df['Entity_Score'] = df['Entity Type'].map(entity_map).fillna(1)

# Has Parent Company
df['Has_Parent'] = df['Parent Company'].notna().astype(int)

# Industry Sector (2-digit SIC)
df['SIC_2Digit'] = df['SIC Code'].astype(str).str[:2]

# Revenue per Employee (productivity indicator)
df['Revenue_Per_Employee'] = np.where(
    df['Employees_Total_Clean'] > 0,
    df['Revenue_USD_Clean'] / df['Employees_Total_Clean'],
    0
)

# Log-transformed features for better clustering
df['Log_Revenue'] = np.log1p(df['Revenue_USD_Clean'])
df['Log_Employees'] = np.log1p(df['Employees_Total_Clean'])

# Data Completeness Score
important_fields = ['Revenue (USD)', 'Employees Total', 'SIC Code', 'Entity Type', 'Region', 'Country']
df['Data_Completeness'] = df[important_fields].notna().sum(axis=1) / len(important_fields)

print(f"   Created {6} new features")

# ============================================================
# 3. INDUSTRY BENCHMARKING
# ============================================================
print("\nðŸ“Š Calculating industry benchmarks...")

# Group by industry sector
industry_stats = df.groupby('SIC_2Digit').agg({
    'Revenue_USD_Clean': ['median', 'mean', 'std', 'count'],
    'Employees_Total_Clean': ['median', 'mean', 'std']
}).reset_index()

industry_stats.columns = ['SIC_2Digit', 'Ind_Revenue_Median', 'Ind_Revenue_Mean', 'Ind_Revenue_Std', 'Ind_Count',
                          'Ind_Employees_Median', 'Ind_Employees_Mean', 'Ind_Employees_Std']

df = df.merge(industry_stats, on='SIC_2Digit', how='left')

# Calculate percentiles within industry
def industry_percentile(row, col):
    if pd.isna(row['Ind_Revenue_Median']):
        return 50  # Default to median if no industry data
    industry_data = df[df['SIC_2Digit'] == row['SIC_2Digit']][col]
    if len(industry_data) == 0:
        return 50
    return (industry_data < row[col]).sum() / len(industry_data) * 100

# Simplified: Compare to industry median
df['Revenue_vs_Industry'] = np.where(
    df['Ind_Revenue_Median'] > 0,
    (df['Revenue_USD_Clean'] / df['Ind_Revenue_Median'] - 1) * 100,
    0
).clip(-100, 500)

df['Employees_vs_Industry'] = np.where(
    df['Ind_Employees_Median'] > 0,
    (df['Employees_Total_Clean'] / df['Ind_Employees_Median'] - 1) * 100,
    0
).clip(-100, 500)

print(f"   Benchmarked against {len(industry_stats)} industry sectors")

# ============================================================
# 4. MULTI-DIMENSIONAL CLUSTERING
# ============================================================
print("\nðŸŽ¯ Performing multi-dimensional clustering...")

# Prepare clustering features
cluster_features = ['Log_Revenue', 'Log_Employees', 'Entity_Score', 'Has_Parent', 'Revenue_Per_Employee']
X_cluster = df[cluster_features].copy()

# Handle infinities and NaNs
X_cluster = X_cluster.replace([np.inf, -np.inf], np.nan)
X_cluster = X_cluster.fillna(X_cluster.median())

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_cluster)

# Find optimal k using silhouette
best_k = 4
best_score = -1
for k in range(3, 8):
    kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels_temp = kmeans_temp.fit_predict(X_scaled)
    score = silhouette_score(X_scaled, labels_temp)
    if score > best_score:
        best_score = score
        best_k = k

print(f"   Optimal clusters: {best_k} (Silhouette: {best_score:.4f})")

# Final clustering
kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)
df['Cluster'] = kmeans.fit_predict(X_scaled)

# Generate cluster names based on characteristics
cluster_profiles = df.groupby('Cluster').agg({
    'Revenue_USD_Clean': 'median',
    'Employees_Total_Clean': 'median',
    'Entity_Score': 'mean',
    'Has_Parent': 'mean'
}).round(2)

def name_cluster(row):
    revenue = row['Revenue_USD_Clean']
    employees = row['Employees_Total_Clean']
    entity = row['Entity_Score']
    has_parent = row['Has_Parent']
    
    size = "Enterprise" if revenue > 1e6 else "SMB" if revenue > 1e4 else "Micro"
    structure = "HQ/Parent" if entity > 2.5 else "Subsidiary" if has_parent > 0.5 else "Independent"
    
    return f"{size} {structure}"

cluster_profiles['Cluster_Name'] = cluster_profiles.apply(name_cluster, axis=1)
df['Cluster_Name'] = df['Cluster'].map(cluster_profiles['Cluster_Name'].to_dict())

print(f"   Cluster distribution:")
for idx, name in cluster_profiles['Cluster_Name'].items():
    count = (df['Cluster'] == idx).sum()
    print(f"      Cluster {idx}: {name} ({count:,} companies)")

# ============================================================
# 5. B2B LEAD SCORE CALCULATION
# ============================================================
print("\nðŸ’° Calculating B2B Lead Scores...")

def calculate_lead_score(row):
    """
    B2B Lead Score (0-100) based on:
    - Revenue potential (40%)
    - Decision-making power (25%)
    - Growth indicators (20%)
    - Data quality (15%)
    """
    score = 0
    
    # 1. Revenue Potential (0-40 points)
    revenue = row['Revenue_USD_Clean']
    if revenue >= 10_000_000:
        score += 40
    elif revenue >= 1_000_000:
        score += 30
    elif revenue >= 100_000:
        score += 20
    elif revenue >= 10_000:
        score += 10
    else:
        score += 5
    
    # 2. Decision-Making Power (0-25 points)
    entity_score = row['Entity_Score']
    score += entity_score * 6.25  # 4 * 6.25 = 25 max
    
    # 3. Growth Indicators (0-20 points)
    # Higher revenue-per-employee = more efficient/growth potential
    rpe = row['Revenue_Per_Employee']
    if rpe >= 500_000:
        score += 20
    elif rpe >= 100_000:
        score += 15
    elif rpe >= 50_000:
        score += 10
    else:
        score += 5
    
    # 4. Data Quality (0-15 points)
    score += row['Data_Completeness'] * 15
    
    return min(100, max(0, score))

df['Lead_Score'] = df.apply(calculate_lead_score, axis=1)

# Lead Score tiers
df['Lead_Tier'] = pd.cut(
    df['Lead_Score'],
    bins=[0, 30, 50, 70, 100],
    labels=['Cold', 'Warm', 'Hot', 'Priority']
)

print(f"   Lead Score distribution:")
print(f"      Priority (70-100): {(df['Lead_Score'] >= 70).sum():,}")
print(f"      Hot (50-70): {((df['Lead_Score'] >= 50) & (df['Lead_Score'] < 70)).sum():,}")
print(f"      Warm (30-50): {((df['Lead_Score'] >= 30) & (df['Lead_Score'] < 50)).sum():,}")
print(f"      Cold (0-30): {(df['Lead_Score'] < 30).sum():,}")

# ============================================================
# 6. RISK SIGNAL DETECTION
# ============================================================
print("\nâš ï¸ Detecting risk signals...")

# Risk 1: Shell company risk (high revenue, zero employees)
df['Risk_Shell'] = (df['Revenue_USD_Clean'] > 100000) & (df['Employees_Total_Clean'] == 0)

# Risk 2: Data inconsistency (missing critical fields)
df['Risk_DataQuality'] = df['Data_Completeness'] < 0.5

# Risk 3: Orphan subsidiary (Subsidiary type but no parent linkage)
df['Risk_OrphanSub'] = (df['Entity Type'] == 'Subsidiary') & (df['Has_Parent'] == 0)

# Isolation Forest for statistical anomalies
iso_features = ['Log_Revenue', 'Log_Employees', 'Revenue_Per_Employee']
X_iso = df[iso_features].replace([np.inf, -np.inf], np.nan).fillna(0)
iso = IsolationForest(contamination=0.05, random_state=42)
df['Anomaly'] = iso.fit_predict(X_iso)
df['Anomaly_Label'] = df['Anomaly'].map({1: 'Normal', -1: 'Anomaly'})

# Combined risk score
df['Risk_Flags'] = df['Risk_Shell'].astype(int) + df['Risk_DataQuality'].astype(int) + df['Risk_OrphanSub'].astype(int) + (df['Anomaly'] == -1).astype(int)

print(f"   Risk signals detected:")
print(f"      Shell company risk: {df['Risk_Shell'].sum():,}")
print(f"      Data quality risk: {df['Risk_DataQuality'].sum():,}")
print(f"      Orphan subsidiary: {df['Risk_OrphanSub'].sum():,}")
print(f"      Statistical anomaly: {(df['Anomaly'] == -1).sum():,}")

# ============================================================
# 7. SAVE ENHANCED RESULTS
# ============================================================
print("\nðŸ’¾ Saving enhanced results...")

output_cols = [
    'DUNS Number ', 'Company Sites', 'Country', 'Region', 'Entity Type',
    'SIC Code', 'SIC Description', 'Employees_Total_Clean', 'Revenue_USD_Clean',
    'Entity_Score', 'Has_Parent', 'Revenue_Per_Employee', 'Data_Completeness',
    'Revenue_vs_Industry', 'Employees_vs_Industry',
    'Cluster', 'Cluster_Name',
    'Lead_Score', 'Lead_Tier',
    'Anomaly_Label', 'Risk_Shell', 'Risk_DataQuality', 'Risk_OrphanSub', 'Risk_Flags'
]

df_output = df[[c for c in output_cols if c in df.columns]]
df_output.to_csv('company_segmentation_results.csv', index=False)

print(f"   Saved {len(df_output):,} companies to company_segmentation_results.csv")

# ============================================================
# 8. SUMMARY
# ============================================================
print("\n" + "=" * 60)
print("ðŸ“ˆ ENHANCED ANALYSIS SUMMARY")
print("=" * 60)
print(f"   Total Companies: {len(df):,}")
print(f"   Clusters: {best_k} (Silhouette: {best_score:.4f})")
print(f"   Industry Sectors: {len(industry_stats)}")
print(f"   Priority Leads: {(df['Lead_Score'] >= 70).sum():,}")
print(f"   High-Risk Entities: {(df['Risk_Flags'] >= 2).sum():,}")
print("=" * 60)
print("âœ… Enhanced analysis complete!")
