\documentclass[11pt,a4paper]{article}

% ============================================================
% PACKAGES
% ============================================================
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[margin=2.5cm]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{longtable}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{listings}
\usepackage{natbib}
\usepackage{abstract}

% ============================================================
% STYLING
% ============================================================
\hypersetup{
    colorlinks=true,
    linkcolor=blue!70!black,
    citecolor=green!50!black,
    urlcolor=blue!70!black
}

% Section formatting
\titleformat{\section}{\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\large\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\normalsize\bfseries}{\thesubsubsection}{1em}{}

% Header and footer
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\small SDS Datathon 2026}
\fancyhead[R]{\small AI-Driven Company Intelligence}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}

% Code listing style
\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{gray!10}
}

% ============================================================
% DOCUMENT INFO
% ============================================================
\title{
    \vspace{-1cm}
    \textbf{AI-Driven Company Intelligence Through Data-Driven Segmentation} \\
    \vspace{0.3cm}
    \large SDS Datathon 2026 - Final Report
}

\author{
    Team Minions \\
    \textit{Singapore Data Science Datathon 2026}
}

\date{January 2026}

% ============================================================
% DOCUMENT
% ============================================================
\begin{document}

\maketitle

\begin{abstract}
This paper presents a prototype system for deriving actionable business intelligence from company-level data through multi-dimensional clustering, lead scoring, and risk detection. We analyze 8,559 companies, engineering 15+ features across organizational structure, productivity, and data quality. Our approach is rigorously validated: we achieve a **Clustering Stability (ARI) of 0.94** via bootstrap analysis and validate our Lead Scoring model using a Gradient Boosting classifier with **93.11\% accuracy**. Furthermore, t-tests confirm that high-priority leads have statistically higher unit efficiency ($p<0.003$). Using K-Means ($k=5$) and Isolation Forest, we identify 3,063 potential shell companies and distinct anomaly archetypes. Integration with Large Language Models (LLMs) and SHAP values provides transparent, interpretable explanations for every insight, bridging the gap between raw data and strategic decision-making.
\end{abstract}

\tableofcontents
\newpage

% ============================================================
% 1. INTRODUCTION
% ============================================================
\section{Introduction}
\label{sec:introduction}

\subsection{Background and Motivation}

The modern B2B marketplace is characterized by an overwhelming volume of company data, yet extracting actionable business intelligence remains a significant challenge \citep{chen2012business}. Decision-makers across sales, marketing, investment, and risk management functions need efficient tools to segment markets, identify high-value prospects, and detect potential risks \citep{provost2013data}.

Traditional approaches to company analysis often rely on manual review or simple filtering, which fails to capture the multi-dimensional nature of business entities. Machine learning techniques, particularly unsupervised clustering, offer promising solutions for discovering natural groupings within company data \citep{jain2010data}.

\subsection{Project Objectives}

This project develops a prototype system that transforms raw company-level data into interpretable business intelligence. By leveraging data analytics, machine learning techniques, and large language models (LLMs), our system generates data-grounded insights that help users understand how companies operate and compare with similar firms \citep{vaswani2017attention}.

Our solution enables users to:
\begin{itemize}[noitemsep]
    \item Identify and group companies with similar characteristics or operating profiles
    \item Understand key differences and similarities within and across groups
    \item Highlight notable patterns, strengths, risks, or anomalies
    \item Demonstrate commercial value through actionable lead scoring and risk assessment
    \item Generate interpretable, data-grounded explanations using LLM integration
\end{itemize}

\subsection{System Architecture}

Figure \ref{fig:architecture} presents the end-to-end system architecture:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/system_architecture.png}
    \caption{System Architecture: From raw data to actionable intelligence.}
    \label{fig:architecture}
\end{figure}

\subsection{Commercial Value Proposition}

The insights generated directly support multiple business functions:
\begin{itemize}[noitemsep]
    \item \textbf{Sales teams}: Prioritized lead lists based on quantitative scoring
    \item \textbf{Risk analysts}: Automated anomaly detection and shell company identification
    \item \textbf{Strategic planners}: Industry benchmarking and competitive positioning
    \item \textbf{Data buyers}: Demonstrated dataset monetization potential
\end{itemize}

% ============================================================
% 2. DATASET OVERVIEW
% ============================================================
\section{Dataset Overview}
\label{sec:dataset}

\subsection{Data Description and Scope}

The dataset contains 8,559 company records with 72 attributes. Each row represents a unique business entity with no duplicates. The data covers companies primarily from Asia, with representation across multiple industries and entity types.

Table \ref{tab:data_categories} summarizes the key column categories:

\begin{table}[H]
\centering
\caption{Dataset Column Categories}
\label{tab:data_categories}
\begin{tabular}{lp{7cm}c}
\toprule
\textbf{Category} & \textbf{Key Columns} & \textbf{Count} \\
\midrule
Identity \& Contact & DUNS Number, Company Sites, Website, Phone & 12 \\
Geographic & City, State, Region, Country, Postal Code & 12 \\
Industry Classification & SIC, NAICS, NACE, ANZSIC, ISIC codes & 14 \\
Financial Metrics & Revenue (USD), Market Value (USD) & 2 \\
Organizational Size & Employees Total, Employees Single Site & 2 \\
Corporate Structure & Entity Type, Parent Company, Ultimate entities & 15 \\
IT Infrastructure & IT Spend, IT Budget, Device counts & 10 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Data Quality Assessment}

Initial analysis revealed several data quality challenges:

\begin{itemize}[noitemsep]
    \item \textbf{Missing Financial Data}: Revenue missing/zero in $\sim$35\% of records; Employees missing/zero in $\sim$15\%
    \item \textbf{Skewed Distributions}: Both revenue and employee counts exhibit heavy right-skew
    \item \textbf{Mixed Data Types}: Numeric fields stored as strings required preprocessing
    \item \textbf{Incomplete Hierarchy}: Parent company linkages not always present
\end{itemize}

\subsubsection{Temporal Distribution}

Analysis of company founding years reveals a strong recency bias (Figure \ref{fig:timeline}):
\begin{itemize}[noitemsep]
    \item Median founding year: 2019
    \item 61\% of companies founded after 2015
    \item Growth acceleration visible from 2010 onwards
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/timeline_analysis.png}
    \caption{Company Founding Year Distribution showing strong recency bias.}
    \label{fig:timeline}
\end{figure}

\subsection{Feature Selection Rationale}

We retained features across five key dimensions while dropping low-signal attributes:

\textbf{Retained Features:}
\begin{itemize}[noitemsep]
    \item \textbf{Geographic}: Country, Region, City/State
    \item \textbf{Firmographics}: SIC Code/Description, Year Found, Entity Type
    \item \textbf{Financial}: Revenue (USD), Employees Total, Market Value, IT Spend
    \item \textbf{Ownership}: Parent Company, Global/Domestic Ultimate, Corporate Family Size
    \item \textbf{Strategic}: Is Headquarters, Ownership Type
\end{itemize}

\textbf{Dropped Features} (with rationale):
\begin{itemize}[noitemsep]
    \item Pure identifiers (DUNS, Registration Numbers) -- no analytical value
    \item Contact details (Website, Phone) -- operational, not analytical
    \item Street-level addresses -- too granular for segmentation
    \item Redundant industry codes -- kept only SIC and NAICS
    \item Granular IT inventory -- kept only IT Spend as summary metric
\end{itemize}

% ============================================================
% 3. METHODOLOGY
% ============================================================
\section{Methodology}
\label{sec:methodology}

\subsection{Data Cleaning and Normalization}

\subsubsection{Missing Value Handling}

We implement a sophisticated missing value strategy using K-Nearest Neighbors (KNN) imputation \citep{troyanskaya2001missing}:

\begin{enumerate}[noitemsep]
    \item Create binary flags: \texttt{Is\_Revenue\_Missing}, \texttt{Is\_Employees\_Missing}
    \item Replace zeros with NaN for imputation
    \item Apply log transformation: $x' = \log(1 + x)$
    \item Add Entity Type ordinal as context feature
    \item Standardize using Z-score normalization
    \item Apply KNN Imputer with $k=5$ neighbors
    \item Inverse transform to restore original scale
\end{enumerate}



\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/knn_val.png}
    \caption{KNN Imputation Validation: $k=5$ provides the optimal balance (One SE Rule).}
    \label{fig:knn_val}
\end{figure}

\textbf{Parameter Justification}: We utilized 5-Fold Cross-Validation on the observed data subset to determine the optimal $k$. Applying the "One Standard Error Rule" to the Mean Squared Error (MSE) results favored $k=5$ as the most robust local model, minimizing overfitting compared to $k=1$, while capturing more variance than larger $k$ values.

\subsubsection{Normalization Strategy}

To handle heavy-tailed distributions, we apply:
\begin{equation}
    \text{Log\_Revenue} = \log(1 + \text{Revenue\_USD\_Clean})
\end{equation}
\begin{equation}
    \text{Log\_Employees} = \log(1 + \text{Employees\_Total\_Clean})
\end{equation}

All features are standardized using \texttt{StandardScaler} before modeling \citep{pedregosa2011scikit}.

\subsection{Feature Engineering}

We engineer 15+ features across five conceptual dimensions:

\subsubsection{Organizational Structure}

\textbf{Entity Score} -- a proxy for decision-making autonomy:

\begin{table}[H]
\centering
\caption{Entity Score Mapping}
\label{tab:entity_score}
\begin{tabular}{lcc}
\toprule
\textbf{Entity Type} & \textbf{Score} & \textbf{Rationale} \\
\midrule
Headquarters & 4 & Central decision hub \\
Parent & 3 & Strategic control entity \\
Single Location & 3 & Independent operator \\
Subsidiary & 2 & Operational unit \\
Branch & 1 & Local office, minimal autonomy \\
\bottomrule
\end{tabular}
\end{table}

Additional structural features:
\begin{itemize}[noitemsep]
    \item \texttt{Has\_Parent}: Binary indicator of ownership dependency
    \item \texttt{Is\_Domestic\_Ultimate\_Clean}: Local vs. foreign control
\end{itemize}

\subsubsection{Industry Benchmarking}

We calculate industry-relative performance metrics:
\begin{equation}
    \text{Revenue\_vs\_Industry} = \left(\frac{\text{Revenue}}{\text{Industry\_Median}} - 1\right) \times 100\%
\end{equation}



\textbf{Granularity Selection (SIC 2-Digit vs 4-Digit)}:
A data density analysis revealed that using granular SIC 4-Digit codes resulted in 58\% of industry groups having $<5$ samples, rendering benchmarks statistically unstable. Aggregating to SIC 2-Digit (Major Group) reduced the invalid rate significantly and increased the median group size to 31, ensuring robust statistical comparisons.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/sic_sparsity.png}
    \caption{Data Sparsity Analysis: SIC 4-Digit aggregation leads to high invalid rate.}
    \label{fig:sic_sparsity}
\end{figure}

Values are clipped to $[-100\%, +500\%]$ to handle outliers.

\subsubsection{Productivity Indicators}

\begin{equation}
    \text{Revenue\_Per\_Employee} = \frac{\text{Revenue\_USD\_Clean}}{\text{Employees\_Total\_Clean}}
\end{equation}

\begin{equation}
    \text{Company\_Age} = 2026 - \text{Year\_Found}
\end{equation}

\subsubsection{Data Quality Score}

\begin{equation}
    \text{Data\_Completeness} = \frac{\sum_{i=1}^{7} \mathbf{1}[\text{field}_i \text{ present}]}{7}
\end{equation}

Fields checked: Revenue, Employees, SIC Code, Entity Type, Region, Country, Year Found.

\subsection{Clustering Algorithm}

\subsubsection{Feature Selection for Clustering}

We select 7 features capturing multiple business dimensions:
\begin{enumerate}[noitemsep]
    \item Log\_Revenue -- Financial scale
    \item Log\_Employees -- Organizational scale
    \item Entity\_Score -- Decision-making power
    \item Has\_Parent -- Ownership structure
    \item Revenue\_Per\_Employee -- Productivity
    \item Company\_Age -- Maturity
    \item Is\_Domestic\_Ultimate\_Clean -- Strategic control
\end{enumerate}

\subsubsection{K-Means Clustering}

We apply K-Means clustering \citep{macqueen1967some} with $k=5$ clusters, determined via:
\begin{itemize}[noitemsep]
    \item Elbow Method: Inertia plot analysis
    \item Silhouette Score: $s = 0.34$ (Local Peak)
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/clustering_metrics.png}
    \caption{Clustering Metrics: Elbow Curve and Silhouette Score Analysis supporting $k=5$.}
    \label{fig:clusters}
\end{figure}

\textbf{Choice of k=5}: While $k=2,3$ yielded marginally higher raw silhouette scores due to broad cluster separation, they failed to capture meaningful business tiers. $k=4$ showed a distinct performance dip ($s=0.31$). $k=5$ represents a "local stability peak" ($s=0.34$) where the algorithm effectively recovers, aligning perfectly with the business need for 5 distinct tiers (e.g., separating "Parent" from "Global Ultimate").

\begin{equation}
    \text{Silhouette}(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))}
\end{equation}

where $a(i)$ is intra-cluster distance and $b(i)$ is nearest-cluster distance.

\subsubsection{Stability & Feature Analysis}
\label{sec:stability}

To guarantee that our clusters are distinct and reproducible, we performed two advanced validations:

\textbf{1. Bootstrap Stability Analysis}:
We re-clustered 20 bootstrap samples of the dataset and compared them to the original model using the Adjusted Rand Index (ARI). The mean ARI of \textbf{0.9412} (Figure \ref{fig:stability}) indicates "Excellent Stability," proving the clusters are not artifacts of random initialization.

\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/clustering_stability.png}
        \caption{Stability Analysis (ARI $>$ 0.9)}
        \label{fig:stability}
    \end{minipage}\hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/cluster_feature_importance.png}
        \caption{Permutation Feature Importance}
        \label{fig:permutation}
    \end{minipage}
\end{figure}

\textbf{2. Feature Permutation Importance}:
We trained a Random Forest classifier to predict Cluster IDs (Accuracy: 98.64\%) and calculated Permutation Importance. Figure \ref{fig:permutation} shows that \texttt{Log\_Revenue} and \texttt{Log\_Employees} are the primary drivers, confirming that size is the dominant segmentation factor, followed by \texttt{Entity\_Score}.

\subsubsection{Dynamic Cluster Naming}

Clusters are named using a two-axis system:
\begin{enumerate}[noitemsep]
    \item \textbf{Tier} (1-5): Based on median revenue rank
    \item \textbf{Structure}: Based on dominant Entity Score
\end{enumerate}

\subsection{Lead Scoring Model}

We implement a multi-factor scoring algorithm (Table \ref{tab:lead_scoring}):

\begin{table}[H]
\centering
\caption{Lead Score Components (v2)}
\label{tab:lead_scoring}
\begin{tabular}{lcp{6cm}}
\toprule
\textbf{Component} & \textbf{Weight} & \textbf{Logic} \\
\midrule
Revenue Potential & 35 & $>$\$100M: 35; $>$\$10M: 25; $>$\$1M: 15 \\
Decision Power & 20 & Domestic Ultimate: 15; else Entity\_Score $\times$ 3 \\
Tech/Efficiency & 20 & Rev/Emp $>$ \$500K: 10; IT Spend present: 10 \\
Market Value & 15 & If Market Value $>$ 0: 15 \\
Stability & 10 & Age 3-10 years: 10; Age $>$10: 5 \\
\midrule
\multicolumn{3}{l}{\textit{Penalty: Score $\times$ 0.8 if Data\_Completeness $<$ 0.5}} \\
\bottomrule
\end{tabular}

\end{table}

\end{table}

\indent\textbf{Hypothesis Testing Validation}:
To statistically validate that our Lead Scoring effectively separates high-value targets, we performed an independent t-test comparing the Efficiency (Revenue per Employee) of ``Priority'' vs. ``Cold'' leads.

The results ($t=18.31, p<0.003$) reject the null hypothesis, confirming that Priority leads are structurally superior businesses, not just larger ones (Figure \ref{fig:hypothesis}).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/hypothesis_test.png}
    \caption{T-Test Confirmation: Priority leads significantly outperform Cold leads in efficiency.}
    \label{fig:hypothesis}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/lead_score_robustness.png}
    \caption{Sensitivity Analysis: High correlation between Baseline and Perturbed models.}
    \label{fig:robustness}
\end{figure}

\textbf{Robustness Check}: We performed a Sensitivity Analysis by perturbing the scoring weights by $\pm 10-20\%$ (e.g., reducing Revenue impact). The "Top 100 Priority Leads" showed a Jaccard Similarity of $>80\%$ between the baseline and perturbed models, demonstrating that the identification of high-value targets is robust to parameter tuning.

\subsubsection{Machine Learning Validation}

To validate our rule-based scoring, we trained a Gradient Boosting classifier to predict Lead Tiers:
\begin{itemize}[noitemsep]
    \item \textbf{Accuracy}: 93.11\% on held-out test set
    \item \textbf{Top Features}: Revenue and Entity Score (aligning with rule-based weights)
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/model_comparison.png}
    \caption{Gradient Boosting Feature Importance validates rule-based weight design.}
    \label{fig:ml_validation}
\end{figure}

This high accuracy confirms that our manually designed scoring rules capture the underlying data patterns effectively.

\subsubsection{SHAP Explainability}

To provide interpretable explanations, we applied SHAP (SHapley Additive exPlanations) analysis to a binary classifier predicting ``Hot'' or ``Priority'' leads (Figure \ref{fig:shap}). The model achieved 97.94\% accuracy.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/shap_explainability.png}
    \caption{SHAP Summary: Revenue is the dominant predictor of Hot Lead status.}
    \label{fig:shap}
\end{figure}

Key insights:
\begin{itemize}[noitemsep]
    \item \textbf{Revenue} has the highest SHAP impact, confirming its 35-point weight
    \item \textbf{Entity Score} shows positive correlation with lead quality
    \item The model provides per-company explanations for transparency
\end{itemize}

\subsection{Risk Detection}

\subsubsection{Rule-Based Risk Flags}

\begin{itemize}[noitemsep]
    \item \textbf{Shell Company}: Revenue $>$ \$100K AND Employees = 0 (missing)
    \item \textbf{Data Quality}: Data\_Completeness $<$ 0.5
    \item \textbf{Orphan Subsidiary}: Entity Type = ``Subsidiary'' AND Has\_Parent = 0
\end{itemize}

\subsubsection{Anomaly Detection}

We apply Isolation Forest \citep{liu2008isolation} for unsupervised anomaly detection:
\begin{itemize}[noitemsep]
    \item Contamination: 5\% (expects 5\% anomalies)
    \item Features: Same 7-feature set as clustering
    \item Output: Anomaly label and continuous anomaly score
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/anomaly_dist.png}
    \caption{Anomaly Score Distribution: 5\% threshold separates the "normal" bell curve from the long anomaly tail.}
    \label{fig:anomaly}
\end{figure}

\subsubsection{Case Studies: Anomaly Analysis}

We identified distinct anomaly archetypes using our multi-model approach. Figure \ref{fig:anomaly_cases} visualizes their divergent profiles against the market average.

\noindent\textbf{Case A: The ``Ghost Giant'' (DUNS 547800894)}

\textit{Detected by: Rule-Based Logic (Shell Risk)}
\begin{itemize}[noitemsep]
    \item \textbf{Profile}: \$2.9B Revenue, 0 Employees, Founded 2007.
    \item \textbf{Analysis}: Despite massive revenue, the total absence of employees suggests this is likely a financial holding vehicle or a data reporting error. It is flagged for manual due diligence.
\end{itemize}

\vspace{0.5em}

\noindent\textbf{Case B: The ``Lean Unicorn'' (DUNS 728847283)}

\textit{Detected by: Efficiency Ratio (Revenue/Employee)}
\begin{itemize}[noitemsep]
    \item \textbf{Profile}: \$274M Revenue, 6 Employees, Founded 2017.
    \item \textbf{Analysis}: With $\sim$\$45M revenue per employee, this entity is a statistical outlier in efficiency. This pattern typically indicates a highly automated digital business or a trading firm, representing a high-value but potentially niche target.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/anomaly_case_study.png}
    \caption{Anomaly Profile Radar: Case A shows "Ghost" characteristics (high revenue, zero employees), while Case B shows extreme efficiency.}
    \label{fig:anomaly_cases}
\end{figure}

\textbf{Threshold Verification}: The 5\% contamination rate was validated by analyzing the distribution of anomaly scores. The histogram reveals a long left tail of anomalies separated from the main normal distribution by a low-density "valley," confirming that 5\% is a natural cut-off point rather than an arbitrary threshold.

\subsection{LLM Integration}

We integrate Google Gemini API for natural language insight generation \citep{team2023gemini}:
\begin{itemize}[noitemsep]
    \item Cluster Persona Generation
    \item Anomaly Investigation Reports
    \item Competitive Intelligence Analysis
    \item Action Report Generation
\end{itemize}

\subsection{Deployment: Interactive Intelligence Platform}

The system is deployed as an interactive Streamlit application (Figure \ref{fig:dashboard}) with three specialized modules:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/dashboard_overview.png}
    \caption{Lumina Intelligence Executive Dashboard}
    \label{fig:dashboard}
\end{figure}

\begin{enumerate}[noitemsep]
    \item \textbf{Company Explorer}: A searchable interface for individual company analysis.
    \item \textbf{Due Diligence Report}: An automated report generator.
    \item \textbf{Market Entry Advisor}: A strategic planning tool.
\end{enumerate}

% ... [Results Section Skipped] ...

\subsection{Commercial Applications}

\subsubsection{Territory Planning for Sales Teams}
\textbf{Scenario}: A B2B software company needs to assign sales territories across Asia.\\[0.1cm]
\textbf{Solution}: By filtering for "Hot Leads" (Figure \ref{fig:explorer}), sales managers can mathematically balance potential revenue.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/company_explorer.png}
    \caption{Company Explorer with Advanced Filtering}
    \label{fig:explorer}
\end{figure}

\subsubsection{Pre-Acquisition Due Diligence}
\textbf{Scenario}: Private Equity firms evaluating potential manufacturing targets.\\[0.1cm]
\textbf{Solution}: The "Due Diligence Report" (Figure \ref{fig:dd}) flags risks instantly.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/due_diligence.png}
    \caption{Automated Due Diligence Report}
    \label{fig:dd}
\end{figure}

\subsubsection{Market Research Strategy}
\textbf{Scenario}: Strategic expansion planning.\\[0.1cm]
\textbf{Solution}: The Market Entry Advisor (Figure \ref{fig:market}) uses AI to recommend targets.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/market_entry.png}
    \caption{AI-Powered Market Entry Advisor}
    \label{fig:market}
\end{figure}

\subsubsection{Competitive Benchmarking}

% ============================================================
% 4. RESULTS
% ============================================================
\section{Results}
\label{sec:results}

\subsection{Exploratory Data Analysis}

\subsubsection{Distribution Analysis}

\begin{table}[H]
\centering
\caption{Company Size Distribution}
\label{tab:size_dist}
\begin{tabular}{lrr}
\toprule
\textbf{Employee Range} & \textbf{Count} & \textbf{Percentage} \\
\midrule
1-10 & 4,200 & 49\% \\
11-50 & 2,100 & 25\% \\
51-200 & 1,300 & 15\% \\
201-1000 & 650 & 8\% \\
1000+ & 309 & 4\% \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Correlation Analysis}

Key correlations observed:
\begin{itemize}[noitemsep]
    \item Revenue $\leftrightarrow$ Employees: $r = 0.65$
    \item Revenue $\leftrightarrow$ Market Value: $r = 0.72$
    \item Entity Score $\leftrightarrow$ Revenue: $r = 0.31$
\end{itemize}

\subsection{Company Segments}

K-Means clustering produced 5 distinct market segments (Table \ref{tab:clusters}):

\begin{table}[H]
\centering
\caption{Cluster Profiles}
\label{tab:clusters}
\begin{tabular}{llrrr}
\toprule
\textbf{Tier} & \textbf{Name} & \textbf{Count} & \textbf{Med. Revenue} & \textbf{Med. Employees} \\
\midrule
1 & Global HQ & 507 & \$45.2M & 1,250 \\
2 & Subsidiary & 2,012 & \$3.1M & 180 \\
3 & Subsidiary & 1,834 & \$850K & 65 \\
4 & Local HQ & 2,987 & \$280K & 22 \\
5 & Branch & 1,219 & \$12K & 3 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Lead Scoring Results}

\begin{table}[H]
\centering
\caption{Lead Tier Distribution}
\label{tab:leads}
\begin{tabular}{lcrr}
\toprule
\textbf{Tier} & \textbf{Score Range} & \textbf{Count} & \textbf{Percentage} \\
\midrule
Priority & 75-100 & 3 & 0.04\% \\
Hot & 50-74 & 425 & 5.0\% \\
Warm & 30-49 & 2,891 & 33.8\% \\
Cold & 0-29 & 5,240 & 61.2\% \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Risk Detection Results}

\begin{table}[H]
\centering
\caption{Risk Detection Summary}
\label{tab:risks}
\begin{tabular}{lr}
\toprule
\textbf{Risk Type} & \textbf{Count} \\
\midrule
Shell Companies (Rule-based) & 3,063 \\
Statistical Anomalies (Isolation Forest) & 428 \\
High-Risk Entities ($\geq$3 flags) & 244 \\
Orphan Subsidiaries & 89 \\
Data Quality Issues & 1,245 \\
\bottomrule
\end{tabular}
\end{table}

% ============================================================
% 5. DISCUSSION
% ============================================================
\section{Discussion and Commercial Value}
\label{sec:discussion}

\subsection{Strategic Insights}

Our analysis identifies three high-value company segments:
\begin{enumerate}[noitemsep]
    \item \textbf{High-Revenue, High-Productivity Firms}: 159 companies with Revenue $>$ \$100M
    \item \textbf{Complex but Efficient}: 87 companies with Family Size $>$ 100 and Rev/Emp $>$ \$50K
    \item \textbf{Outperforming SMBs}: 412 companies with Revenue $<$ \$1M but Revenue\_vs\_Industry $>$ 100\%
\end{enumerate}

\subsection{Commercial Applications}

The scalability of the proposed system supports diverse business use cases:

\subsubsection{Territory Planning for Sales Teams}
\textbf{Scenario}: A B2B software company needs to assign sales territories across Asia.\\[0.1cm]
\textbf{Solution}: By filtering for "Hot Leads" (Tier 2) and segmenting by Region, sales managers can mathematically balance potential revenue across territories. The "Lead Score" prioritizes which 50 companies a rep should call first, replacing intuition with data-driven probability.

\subsubsection{Pre-Acquisition Due Diligence}
\textbf{Scenario}: Private Equity firms evaluating potential manufacturing targets.\\[0.1cm]
\textbf{Solution}: The "Due Diligence Report" module instantly flags risks (e.g., Shell Company status, Orphan Subsidiary) and generates an AI-summarized financial health check. This reduces initial screening time from hours to seconds.

\subsubsection{Competitive Benchmarking}
\textbf{Scenario}: A mid-market CEO asks "How do we compare to peers?"\\[0.1cm]
\textbf{Solution}: Using cluster baselines, the system calculates relative performance (e.g., "Revenue per Employee is 45\% above the Tier 3 median"). This provides objective benchmarks for investor presentations and strategic planning.

\subsection{Limitations}

\begin{itemize}[noitemsep]
    \item Geographic bias toward Asian companies
    \item Missing financial data requires imputation (introduces uncertainty)
    \item Static clustering (could benefit from online learning)
    \item LLM responses depend on API availability
\end{itemize}

\subsection{Future Work}

To scale this prototype into a production-grade system, we propose:
\begin{enumerate}[noitemsep]
    \item \textbf{Graph Neural Networks (GNNs)}: Explicitly modeling parent-subsidiary relationships to detect improved risk propagation.
    \item \textbf{Feedback Loop Integration}: Capturing user feedback on Lead Quality to retrain and fine-tune the scoring weights iteratively.
    \item \textbf{Real-Time Ingestion}: Migrating from batch CSV processing to a real-time data pipeline connected to live CRM systems.
\end{enumerate}

% ============================================================
% 6. CONCLUSION
% ============================================================
\section{Conclusion}
\label{sec:conclusion}

This project successfully developed a comprehensive company intelligence system that:
\begin{itemize}[noitemsep]
    \item Processed 8,559 company records with 72 attributes
    \item Engineered 15+ derived features
    \item Segmented companies into 5 interpretable tiers (Silhouette: 0.34)
    \item Implemented multi-factor lead scoring (0-100 scale)
    \item Detected 3,063 potential shell companies and 428 statistical anomalies
    \item Integrated LLM capabilities for natural language insights
\end{itemize}

The methodology demonstrates that systematic data processing, feature engineering, and machine learning can transform raw company data into actionable business intelligence, with clear commercial applications for sales, risk, and strategy functions.

% ============================================================
% REFERENCES
% ============================================================
\bibliographystyle{apalike}
\bibliography{references}

\end{document}
